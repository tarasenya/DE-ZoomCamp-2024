{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, types\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: \n",
    "\n",
    "**FHV October 2019**\n",
    "\n",
    "Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons.\n",
    "\n",
    "Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches.\n",
    "\n",
    "- 1MB\n",
    "- 6MB\n",
    "- 25MB\n",
    "- 87MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/28 14:49:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/28 14:49:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName('hw').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv('fhv_data/fhv_tripdata_2019-10.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropOff_datetime', StringType(), True), StructField('PUlocationID', StringType(), True), StructField('DOlocationID', StringType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType(\n",
    "    [types.StructField('dispatching_base_num', types.StringType(), True), \n",
    "     types.StructField('pickup_datetime', types.TimestampType(), True), \n",
    "     types.StructField('dropOff_datetime', types.TimestampType(), True), \n",
    "     types.StructField('PUlocationID', types.IntegerType(), True), \n",
    "     types.StructField('DOlocationID', types.IntegerType(), True), \n",
    "     types.StructField('SR_Flag', types.StringType(), True), \n",
    "     types.StructField('Affiliated_base_number', types.StringType(), True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").schema(schema).csv('fhv_data/fhv_tripdata_2019-10.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   null|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   null|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|         264|         264|   null|                B00014|\n",
      "|     B00021         |2019-10-01 00:00:48|2019-10-01 00:07:12|         129|         129|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:47:23|2019-10-01 00:53:25|          57|          57|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:10:06|2019-10-01 00:19:50|         173|         173|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:51:37|2019-10-01 01:06:14|         226|         226|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:28:23|2019-10-01 00:34:33|          56|          56|   null|       B00021         |\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(6).write.parquet('fhv_data/2019/',mode='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 39M\n",
      "drwxr-xr-x 2 taras taras 4.0K Feb 28 12:27 .\n",
      "drwxrwxr-x 3 taras taras 4.0K Feb 28 12:27 ..\n",
      "-rw-r--r-- 1 taras taras    8 Feb 28 12:27 ._SUCCESS.crc\n",
      "-rw-r--r-- 1 taras taras  51K Feb 28 12:27 .part-00000-127a01a6-d78a-47d9-901f-bf28088f48b5-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 taras taras  51K Feb 28 12:27 .part-00001-127a01a6-d78a-47d9-901f-bf28088f48b5-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 taras taras  51K Feb 28 12:27 .part-00002-127a01a6-d78a-47d9-901f-bf28088f48b5-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 taras taras  51K Feb 28 12:27 .part-00003-127a01a6-d78a-47d9-901f-bf28088f48b5-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 taras taras  51K Feb 28 12:27 .part-00004-127a01a6-d78a-47d9-901f-bf28088f48b5-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 taras taras  51K Feb 28 12:27 .part-00005-127a01a6-d78a-47d9-901f-bf28088f48b5-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 taras taras    0 Feb 28 12:27 _SUCCESS\n",
      "-rw-r--r-- 1 taras taras 6.4M Feb 28 12:27 part-00000-127a01a6-d78a-47d9-901f-bf28088f48b5-c000.snappy.parquet\n",
      "-rw-r--r-- 1 taras taras 6.4M Feb 28 12:27 part-00001-127a01a6-d78a-47d9-901f-bf28088f48b5-c000.snappy.parquet\n",
      "-rw-r--r-- 1 taras taras 6.4M Feb 28 12:27 part-00002-127a01a6-d78a-47d9-901f-bf28088f48b5-c000.snappy.parquet\n",
      "-rw-r--r-- 1 taras taras 6.4M Feb 28 12:27 part-00003-127a01a6-d78a-47d9-901f-bf28088f48b5-c000.snappy.parquet\n",
      "-rw-r--r-- 1 taras taras 6.4M Feb 28 12:27 part-00004-127a01a6-d78a-47d9-901f-bf28088f48b5-c000.snappy.parquet\n",
      "-rw-r--r-- 1 taras taras 6.4M Feb 28 12:27 part-00005-127a01a6-d78a-47d9-901f-bf28088f48b5-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lah fhv_data/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: \n",
    "\n",
    "How many taxi trips were there on the 15th of October?\n",
    "\n",
    "Consider only trips that started on the 15th of October.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62610"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn('pickup_date', F.to_date(df.pickup_datetime)).\\\n",
    "    filter(\"pickup_date = '2019-10-15'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: \n",
    "\n",
    "**Longest trip for each day** \n",
    "\n",
    "What is the length of the longest trip in the dataset in hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|pickup_date|    max(duration)|\n",
      "+-----------+-----------------+\n",
      "| 2019-10-28|         631152.5|\n",
      "| 2019-10-11|         631152.5|\n",
      "| 2019-10-31|87672.44083333333|\n",
      "| 2019-10-01|70128.02805555555|\n",
      "| 2019-10-17|           8794.0|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.withColumn('duration', (df.dropOff_datetime.cast('long')\n",
    "              -df.pickup_datetime.cast('long'))/3600).\\\n",
    "              withColumn('pickup_date', F.to_date(df.pickup_datetime)).\\\n",
    "              groupBy('pickup_date').max('duration').orderBy('max(duration)', ascending=False).limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: \n",
    "\n",
    "**Least frequent pickup location zone**\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark</br>\n",
    "[Zone Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv)\n",
    "\n",
    "Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_data = spark.read.option(\"header\", \"true\").csv('fhv_data/taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('LocationID', StringType(), True), StructField('Borough', StringType(), True), StructField('Zone', StringType(), True), StructField('service_zone', StringType(), True)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zone_data.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_schema = types.StructType([\n",
    "    types.StructField('LocationID', types.IntegerType(), True), \n",
    "    types.StructField('Borough', types.StringType(), True), \n",
    "    types.StructField('Zone', types.StringType(), True), \n",
    "    types.StructField('service_zone', types.StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_data = spark.read.schema(zone_schema).option(\"header\", \"true\").csv('fhv_data/taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zone_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   null|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   null|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|         264|         264|   null|                B00014|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('fhv_data')\n",
    "zone_data.createOrReplaceTempView('zone_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|                zone|freq|\n",
      "+--------------------+----+\n",
      "|         Jamaica Bay|   1|\n",
      "|Governor's Island...|   2|\n",
      "| Green-Wood Cemetery|   5|\n",
      "|       Broad Channel|   8|\n",
      "|     Highbridge Park|  14|\n",
      "|        Battery Park|  15|\n",
      "|Saint Michaels Ce...|  23|\n",
      "|Breezy Point/Fort...|  25|\n",
      "|Marine Park/Floyd...|  26|\n",
      "|        Astoria Park|  29|\n",
      "+--------------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT zone_data.Zone as zone,\n",
    "          count(*) as freq\n",
    "FROM fhv_data\n",
    "INNER JOIN zone_data\n",
    "ON fhv_data.PULocationID = zone_data.LocationID\n",
    "GROUP BY \n",
    "          zone_data.Zone\n",
    "ORDER BY \n",
    "          2 ASC\n",
    "LIMIT 10;          \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DE-ZoomCamp-2024-X3ICOgdM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
